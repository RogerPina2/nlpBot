{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "233cede5-2122-4619-aa4d-bdc40327e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import urllib\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.utils import pad_sequences\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://gist.githubusercontent.com/alopes/5358189/raw/2107d809cca6b83ce3d8e04dbd9463283025284f/stopwords.txt\"\n",
    "stopwords_list = urllib.request.urlopen(url).read().decode()\n",
    "stopwords_ptbr = set(stopwords_list.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a6c172e-0a50-4cf3-b4ae-32fe1d2d8525",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/database.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "677ce61b-60c1-4412-881b-c5239ea8be89",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['content'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d5dcdef-00df-4e84-acf0-45ce02431f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 0\n",
    "for t in texts:\n",
    "    if len(t) > max_words:\n",
    "        max_words = len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c34f8bb-d5f4-444f-bbc9-622731d77466",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94a14fca-f400-4ae3-8ee8-c55a35889c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [item for sublist in sequences for item in sublist]\n",
    "vocab_size = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3381e676-8f74-4505-9412-9ef633545724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on 19 words to predict the 20th\n",
    "sentence_len = 20\n",
    "pred_len = 1\n",
    "train_len = sentence_len - pred_len\n",
    "seq = []\n",
    "# Sliding window to generate train data\n",
    "for i in range(len(text)-sentence_len):\n",
    "    seq.append(text[i:i+sentence_len])\n",
    "# Reverse dictionary to decode tokenized sequences back to words\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3d2444b-d002-4c1a-b44e-a6b5df256cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each row in seq is a 20 word long window. We append he first 19 words as the input to predict the 20th word\n",
    "trainX = []\n",
    "trainy = []\n",
    "for i in seq:\n",
    "    trainX.append(i[:train_len])\n",
    "    trainy.append(i[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86039bc9-d789-4a92-af4b-db9500982da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2965/2965 [==============================] - ETA: 0s - loss: 6.6187 - accuracy: 0.0677\n",
      "Epoch 1: loss improved from inf to 6.61868, saving model to .\\model_2_weights.hdf5\n",
      "2965/2965 [==============================] - 363s 119ms/step - loss: 6.6187 - accuracy: 0.0677\n",
      "Epoch 2/20\n",
      "2965/2965 [==============================] - ETA: 0s - loss: 5.1696 - accuracy: 0.2083\n",
      "Epoch 2: loss improved from 6.61868 to 5.16959, saving model to .\\model_2_weights.hdf5\n",
      "2965/2965 [==============================] - 363s 122ms/step - loss: 5.1696 - accuracy: 0.2083\n",
      "Epoch 3/20\n",
      "2965/2965 [==============================] - ETA: 0s - loss: 4.4369 - accuracy: 0.3470\n",
      "Epoch 3: loss improved from 5.16959 to 4.43686, saving model to .\\model_2_weights.hdf5\n",
      "2965/2965 [==============================] - 322s 108ms/step - loss: 4.4369 - accuracy: 0.3470\n",
      "Epoch 4/20\n",
      "2965/2965 [==============================] - ETA: 0s - loss: 4.0810 - accuracy: 0.3992\n",
      "Epoch 4: loss improved from 4.43686 to 4.08097, saving model to .\\model_2_weights.hdf5\n",
      "2965/2965 [==============================] - 364s 123ms/step - loss: 4.0810 - accuracy: 0.3992\n",
      "Epoch 5/20\n",
      "2965/2965 [==============================] - ETA: 0s - loss: 3.8327 - accuracy: 0.4254\n",
      "Epoch 5: loss improved from 4.08097 to 3.83275, saving model to .\\model_2_weights.hdf5\n",
      "2965/2965 [==============================] - 344s 116ms/step - loss: 3.8327 - accuracy: 0.4254\n",
      "Epoch 6/20\n",
      "2965/2965 [==============================] - ETA: 0s - loss: 3.6438 - accuracy: 0.4407\n",
      "Epoch 6: loss improved from 3.83275 to 3.64379, saving model to .\\model_2_weights.hdf5\n",
      "2965/2965 [==============================] - 281s 95ms/step - loss: 3.6438 - accuracy: 0.4407\n",
      "Epoch 7/20\n",
      "2965/2965 [==============================] - ETA: 0s - loss: 3.4799 - accuracy: 0.4556\n",
      "Epoch 7: loss improved from 3.64379 to 3.47986, saving model to .\\model_2_weights.hdf5\n",
      "2965/2965 [==============================] - 278s 94ms/step - loss: 3.4799 - accuracy: 0.4556\n",
      "Epoch 8/20\n",
      "2965/2965 [==============================] - ETA: 0s - loss: 3.3320 - accuracy: 0.4673\n",
      "Epoch 8: loss improved from 3.47986 to 3.33197, saving model to .\\model_2_weights.hdf5\n",
      "2965/2965 [==============================] - 280s 95ms/step - loss: 3.3320 - accuracy: 0.4673\n",
      "Epoch 9/20\n",
      "2965/2965 [==============================] - ETA: 0s - loss: 3.1987 - accuracy: 0.4781\n",
      "Epoch 9: loss improved from 3.33197 to 3.19871, saving model to .\\model_2_weights.hdf5\n",
      "2965/2965 [==============================] - 258s 87ms/step - loss: 3.1987 - accuracy: 0.4781\n",
      "Epoch 10/20\n",
      "2965/2965 [==============================] - ETA: 0s - loss: 3.0804 - accuracy: 0.4869\n",
      "Epoch 10: loss improved from 3.19871 to 3.08041, saving model to .\\model_2_weights.hdf5\n",
      "2965/2965 [==============================] - 258s 87ms/step - loss: 3.0804 - accuracy: 0.4869\n",
      "Epoch 11/20\n",
      "2965/2965 [==============================] - ETA: 0s - loss: 2.9683 - accuracy: 0.4956\n",
      "Epoch 11: loss improved from 3.08041 to 2.96827, saving model to .\\model_2_weights.hdf5\n",
      "2965/2965 [==============================] - 278s 94ms/step - loss: 2.9683 - accuracy: 0.4956\n",
      "Epoch 12/20\n",
      "2965/2965 [==============================] - ETA: 0s - loss: 2.8633 - accuracy: 0.5043\n",
      "Epoch 12: loss improved from 2.96827 to 2.86331, saving model to .\\model_2_weights.hdf5\n",
      "2965/2965 [==============================] - 284s 96ms/step - loss: 2.8633 - accuracy: 0.5043\n",
      "Epoch 13/20\n",
      "2965/2965 [==============================] - ETA: 0s - loss: 2.7676 - accuracy: 0.5123\n",
      "Epoch 13: loss improved from 2.86331 to 2.76756, saving model to .\\model_2_weights.hdf5\n",
      "2965/2965 [==============================] - 281s 95ms/step - loss: 2.7676 - accuracy: 0.5123\n",
      "Epoch 14/20\n",
      "2965/2965 [==============================] - ETA: 0s - loss: 2.6793 - accuracy: 0.5197\n",
      "Epoch 14: loss improved from 2.76756 to 2.67925, saving model to .\\model_2_weights.hdf5\n",
      "2965/2965 [==============================] - 260s 88ms/step - loss: 2.6793 - accuracy: 0.5197\n",
      "Epoch 15/20\n",
      "2965/2965 [==============================] - ETA: 0s - loss: 2.5939 - accuracy: 0.5273\n",
      "Epoch 15: loss improved from 2.67925 to 2.59389, saving model to .\\model_2_weights.hdf5\n",
      "2965/2965 [==============================] - 259s 87ms/step - loss: 2.5939 - accuracy: 0.5273\n",
      "Epoch 16/20\n",
      "2965/2965 [==============================] - ETA: 0s - loss: 2.5167 - accuracy: 0.5338\n",
      "Epoch 16: loss improved from 2.59389 to 2.51673, saving model to .\\model_2_weights.hdf5\n",
      "2965/2965 [==============================] - 261s 88ms/step - loss: 2.5167 - accuracy: 0.5338\n",
      "Epoch 17/20\n",
      "2965/2965 [==============================] - ETA: 0s - loss: 2.4424 - accuracy: 0.5416\n",
      "Epoch 17: loss improved from 2.51673 to 2.44235, saving model to .\\model_2_weights.hdf5\n",
      "2965/2965 [==============================] - 307s 104ms/step - loss: 2.4424 - accuracy: 0.5416\n",
      "Epoch 18/20\n",
      "2965/2965 [==============================] - ETA: 0s - loss: 2.3007 - accuracy: 0.5552\n",
      "Epoch 19: loss improved from 2.37170 to 2.30072, saving model to .\\model_2_weights.hdf5\n",
      "2965/2965 [==============================] - 575s 194ms/step - loss: 2.3007 - accuracy: 0.5552\n",
      "Epoch 20/20\n",
      "2965/2965 [==============================] - ETA: 0s - loss: 2.2357 - accuracy: 0.5637\n",
      "Epoch 20: loss improved from 2.30072 to 2.23575, saving model to .\\model_2_weights.hdf5\n",
      "2965/2965 [==============================] - 531s 179ms/step - loss: 2.2357 - accuracy: 0.5637\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model_2 = Sequential([\n",
    "    Embedding(vocab_size+1, 50, input_length=train_len),\n",
    "    LSTM(100, return_sequences=True),\n",
    "    LSTM(100),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "# Train model with checkpoints\n",
    "model_2.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "filepath = \"./model_2_weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "history = model_2.fit(np.asarray(trainX),\n",
    "         pd.get_dummies(np.asarray(trainy)),\n",
    "         epochs = 20,\n",
    "         batch_size = 64,\n",
    "         callbacks = callbacks_list,\n",
    "         verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17e4c79b-c0fd-445b-a63f-96facf9ae1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(model,seq,max_len = 10):\n",
    "    tokenized_sent = tokenizer.texts_to_sequences([seq])\n",
    "    max_len = max_len+len(tokenized_sent[0])\n",
    "\n",
    "    while len(tokenized_sent[0]) < max_len:\n",
    "        padded_sentence = pad_sequences(tokenized_sent[-19:],maxlen=19)\n",
    "        op = model.predict(np.asarray(padded_sentence).reshape(1,-1))\n",
    "        tokenized_sent[0].append(op.argmax()+1)\n",
    "        \n",
    "    return \" \".join(map(lambda x : reverse_word_map[x],tokenized_sent[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa1525c3-3dfb-4a23-90ff-fb1845be75ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'o presidente da bbc news brasil ensaiado disse barrett a bbc co'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(model_2, \"O presidente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3728e149-38cd-4361-951f-840d9b318cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/model_gerador.joblib']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model to a file\n",
    "joblib.dump(model_2, '../models/model_gerador.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c64f31f2-8b00-4030-998b-ce384bd3499f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/tokenizer_gen.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(tokenizer, '../models/tokenizer_gen.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0cd0c3-1888-414f-9dc8-dcca86d0ac00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
